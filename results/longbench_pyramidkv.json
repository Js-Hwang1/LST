{
  "model": "mistralai/Mistral-7B-Instruct-v0.2",
  "tasks": [
    "qasper",
    "multifieldqa_en",
    "hotpotqa",
    "2wikimqa",
    "musique",
    "gov_report",
    "multi_news",
    "trec",
    "triviaqa",
    "samsum",
    "lcc",
    "repobench-p"
  ],
  "methods": [
    "pyramidkv"
  ],
  "results": {
    "pyramidkv": {
      "qasper": {
        "score": 6.64,
        "n_samples": 100,
        "metric": "f1"
      },
      "multifieldqa_en": {
        "score": 13.16,
        "n_samples": 100,
        "metric": "f1"
      },
      "hotpotqa": {
        "score": 2.96,
        "n_samples": 100,
        "metric": "f1"
      },
      "2wikimqa": {
        "score": 7.56,
        "n_samples": 100,
        "metric": "f1"
      },
      "musique": {
        "score": 2.3,
        "n_samples": 100,
        "metric": "f1"
      },
      "gov_report": {
        "score": 12.05,
        "n_samples": 100,
        "metric": "rouge"
      },
      "multi_news": {
        "score": 16.59,
        "n_samples": 100,
        "metric": "rouge"
      },
      "trec": {
        "score": 0.0,
        "n_samples": 100,
        "metric": "classification"
      },
      "triviaqa": {
        "score": 10.62,
        "n_samples": 100,
        "metric": "f1"
      },
      "samsum": {
        "score": 5.47,
        "n_samples": 100,
        "metric": "rouge"
      },
      "lcc": {
        "score": 23.73,
        "n_samples": 100,
        "metric": "code_sim"
      },
      "repobench-p": {
        "score": 22.19,
        "n_samples": 100,
        "metric": "code_sim"
      },
      "average": 10.27
    }
  },
  "note": "Scores are in percentage (0-100) per LongBench standard"
}