{
  "model": "mistralai/Mistral-7B-Instruct-v0.2",
  "tasks": [
    "qasper",
    "multifieldqa_en",
    "hotpotqa",
    "2wikimqa",
    "musique",
    "gov_report",
    "multi_news",
    "trec",
    "triviaqa",
    "samsum",
    "lcc",
    "repobench-p"
  ],
  "methods": [
    "dense"
  ],
  "results": {
    "dense": {
      "qasper": {
        "score": 28.82,
        "n_samples": 100,
        "metric": "f1"
      },
      "multifieldqa_en": {
        "score": 49.53,
        "n_samples": 100,
        "metric": "f1"
      },
      "hotpotqa": {
        "score": 37.09,
        "n_samples": 100,
        "metric": "f1"
      },
      "2wikimqa": {
        "score": 22.74,
        "n_samples": 100,
        "metric": "f1"
      },
      "musique": {
        "score": 18.47,
        "n_samples": 100,
        "metric": "f1"
      },
      "gov_report": {
        "score": 33.28,
        "n_samples": 100,
        "metric": "rouge"
      },
      "multi_news": {
        "score": 26.81,
        "n_samples": 100,
        "metric": "rouge"
      },
      "trec": {
        "score": 0.0,
        "n_samples": 100,
        "metric": "classification"
      },
      "triviaqa": {
        "score": 73.68,
        "n_samples": 100,
        "metric": "f1"
      },
      "samsum": {
        "score": 38.71,
        "n_samples": 100,
        "metric": "rouge"
      },
      "lcc": {
        "score": 33.83,
        "n_samples": 100,
        "metric": "code_sim"
      },
      "repobench-p": {
        "score": 31.65,
        "n_samples": 100,
        "metric": "code_sim"
      },
      "average": 32.88
    }
  },
  "note": "Scores are in percentage (0-100) per LongBench standard"
}